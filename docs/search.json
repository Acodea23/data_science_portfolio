[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Welcome to my data science portfolio! This site shows my journey learning data science and analytics. On this website you will find past data projects that focus on statistical skills in R, and current project that focs on Data Science skills in Python. I hope you find something that interests you and reach out to chat about my work!\n\n\nThis portfolio shows my work learning data science. My goal is to help you to understand the skills I would offer as an employee. Each project includes:\n\nMy code with documentation\nVisualizations I created\nWhat I learned and discovered\n\nI built this site using Quarto and host it on GitHub Pages.\n\n\n\n\nProgramming: Python, Pandas for data analysis\nVisualization: Creating charts with Matplotlib and Seaborn\nData Collection: Getting data from files, websites, and APIs\nAnalysis: Finding patterns and answering questions with data\n\n\n\n\n\n\n\nA blog post about accounting for model sensitivity\n\n\n\nLearn how I explore datasets to find interesting patterns and answer questions.\n\n\n\nSee how I gather data from different sources and prepare it for analysis.\n\n\n\nSee how I tackle a data science project beginning to end.\n\n\n\nThanks for visiting! Feel free to explore my projects and see what I’m learning."
  },
  {
    "objectID": "index.html#about-this-portfolio",
    "href": "index.html#about-this-portfolio",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "This portfolio shows my work learning data science. My goal is to help you to understand the skills I would offer as an employee. Each project includes:\n\nMy code with documentation\nVisualizations I created\nWhat I learned and discovered\n\nI built this site using Quarto and host it on GitHub Pages."
  },
  {
    "objectID": "index.html#skills-im-learning",
    "href": "index.html#skills-im-learning",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Programming: Python, Pandas for data analysis\nVisualization: Creating charts with Matplotlib and Seaborn\nData Collection: Getting data from files, websites, and APIs\nAnalysis: Finding patterns and answering questions with data"
  },
  {
    "objectID": "index.html#my-projects",
    "href": "index.html#my-projects",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "A blog post about accounting for model sensitivity\n\n\n\nLearn how I explore datasets to find interesting patterns and answer questions.\n\n\n\nSee how I gather data from different sources and prepare it for analysis.\n\n\n\nSee how I tackle a data science project beginning to end.\n\n\n\nThanks for visiting! Feel free to explore my projects and see what I’m learning."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects Overview",
    "section": "",
    "text": "Description: A blog post about accounting for model sensitivity\n\n\n\nDescription: Pick a dataset and explore it to discover insights and answer questions.\n\n\n\nDescription: Find an interesting data source, collect the data, and prepare it for analysis.\n\n\n\nDescription: A comprehensive project that shows off my data science skills.",
    "crumbs": [
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/index.html#all-projects",
    "href": "projects/index.html#all-projects",
    "title": "Projects Overview",
    "section": "",
    "text": "Description: A blog post about accounting for model sensitivity\n\n\n\nDescription: Pick a dataset and explore it to discover insights and answer questions.\n\n\n\nDescription: Find an interesting data source, collect the data, and prepare it for analysis.\n\n\n\nDescription: A comprehensive project that shows off my data science skills.",
    "crumbs": [
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/eda.html",
    "href": "projects/eda.html",
    "title": "EDA Project",
    "section": "",
    "text": "This is coming down the pipeline. Check again later.",
    "crumbs": [
      "EDA Project"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "A picture of me smiling at the beach"
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About Me",
    "section": "",
    "text": "Write a brief introduction about yourself here. Include:\n\nYour academic background\nYour interest in data science\nYour career goals\nAny relevant experience or projects"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\nBS Statistics, Minor Mathematics - Brigham Young University, Apr 2027\nRelevant Coursework: Data Science Process, Machine Learning, Applied Programming, Linear Regression, Probability and Inference, Statistical Modeling, Bayesian Reliability, Spanish"
  },
  {
    "objectID": "about.html#skills-interests",
    "href": "about.html#skills-interests",
    "title": "About Me",
    "section": "Skills & Interests",
    "text": "Skills & Interests\nLinear Regression, Model Training, Prediction, Longitudinal Data, Model Evaluation, Bayesian Clustering\n\nTechnical Skills\n\nProgramming: Python, SQL, R, Excel\nData Analysis: Pandas, NumPy\nVisualization: Matplotlib, Seaborn, Tableau, gplot2\nMachine Learning: Scikit-learn\nTools: Jupyter Notebooks, Git/GitHub\n\n\n\nAreas of Interest\n\nMy main interest is applying statistics to real-world problems. Whatever topic you are passionate about, I want to use data to help you understand it better.\nSome areas I’m curious about are agriculture, business, engineering, and sports."
  },
  {
    "objectID": "about.html#goals",
    "href": "about.html#goals",
    "title": "About Me",
    "section": "Goals",
    "text": "Goals\n\nUnderstand machine learning algorithms better\nPublish an academic paper\nWork as a data science intern"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About Me",
    "section": "Contact",
    "text": "Contact\n\nEmail: wilson.matthewg@gmail.com\nGitHub: Acodea23\nLinkedIn: MatthewWilson01\n\n\nThis portfolio showcases my learning progress and projects completed during my data science studies. It will be updated over the course of the semester"
  },
  {
    "objectID": "projects/data-acquisition.html",
    "href": "projects/data-acquisition.html",
    "title": "Data Acquisition",
    "section": "",
    "text": "I find it very important to understand how the circumstances of someone’s life impacts their health. I am in a human development class where we are learning about how the factors of our lives impact us. Both the factors we can and can’t control. In this dataset, I have the chance to study how a person’s race and sex (things they can’t control) and a person’s educational status (something partially controllable) change how likely they were to be killed by covid. Or to see if it in fact has no impact.",
    "crumbs": [
      "Data Acquisition"
    ]
  },
  {
    "objectID": "projects/final-project.html",
    "href": "projects/final-project.html",
    "title": "Final Project",
    "section": "",
    "text": "This is coming down the pipeline. Check again later.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "projects/blog_post.html",
    "href": "projects/blog_post.html",
    "title": "How to fix (model) sensitivity problems",
    "section": "",
    "text": "When learning to build statistical models, the moment the model “fits” the data and your assumptions are met you tend to pat yourself on the back and call it a day. But in the wild, models need to be flexible. They need to fit not just the data present, but future data. We need to make predictions with this model. We will consider how model sensitivity relates to these flexibility."
  },
  {
    "objectID": "about.html#tldr",
    "href": "about.html#tldr",
    "title": "About Me",
    "section": "TLDR",
    "text": "TLDR\nI’m from Texas and am studying statistics with a minor in mathematics at Brigham Young University. I have co-authored an academic paper in math, and I am currently working on another paper in the field of statistics. Some projects I’ve worked on include metal fatigue analysis, and vaccine trends based on political affiliation. I want to continue to develop models and statistical processes for complex problems. I would love to chat about your favorite data science projects!"
  },
  {
    "objectID": "blog_post.html",
    "href": "blog_post.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nHow to handle (model) sensitivity\nwords to know: scalable, bayesian, prior\nWhen learning to build statisitcal models, the moment the model “fits” the data and your assumptions are met you tend to pat yourself on the back and call it a day. But in the wild, models need to be scalable. They need to be resistant to our biases potentially bleeding into the model. We will consider how sensitivity relates to these 2 topics.\n\n\nScalability"
  },
  {
    "objectID": "blog_post.html#words-to-know-scalable-bayesian-prior",
    "href": "blog_post.html#words-to-know-scalable-bayesian-prior",
    "title": "",
    "section": "words to know: scalable, bayesian, prior",
    "text": "words to know: scalable, bayesian, prior\nWhen learning to build statisitcal models, the moment the model “fits” the data and your assumptions are met you tend to pat yourself on the back and call it a day. But in the wild, models need to be scalable’#Scalability’. They need to be resistant to our biases potentially bleeding into the model. We will consider how sensitivity relates to these 2 topics."
  },
  {
    "objectID": "projects/blog_post.html#problem",
    "href": "projects/blog_post.html#problem",
    "title": "",
    "section": "Problem",
    "text": "Problem"
  },
  {
    "objectID": "projects/blog_post.html#potential-fix",
    "href": "projects/blog_post.html#potential-fix",
    "title": "",
    "section": "Potential Fix",
    "text": "Potential Fix"
  },
  {
    "objectID": "projects/blog_post.html#flexibility",
    "href": "projects/blog_post.html#flexibility",
    "title": "How to fix (model) sensitivity problems",
    "section": "Flexibility",
    "text": "Flexibility\nLets consider these two models. Both of them predict the data. Both of them accuratly model the data, but how well will they model another data point from the same sample? p.s. The code is provided in case you would like to experiment with this youself.\n\n\nCode\n# This code was produced with the aid of Microsoft copilot\nlibrary(ggplot2)\nlibrary(splines)\nlibrary(gridExtra)\n\n# Original data\ndf &lt;- data.frame(x = c(1, 2, 3, 4), y = c(2, 3, 2.5, 3.5))\n\n# Spline data\nspline_df &lt;- data.frame(x = seq(1, 4, length.out = 100))\nspline_df$y &lt;- predict(smooth.spline(df$x, df$y, df = 4), spline_df$x)$y\n\n# First plot: original model fit\np1 &lt;- ggplot(df, aes(x, y)) +\n  geom_point(color = \"black\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", formula = y ~ x) +\n  geom_line(data = spline_df, aes(x, y), color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Original Model Fit\", x = \"X\", y = \"Y\") +\n  theme_minimal()\n\n# Second plot: with new point\ndf_new_point &lt;- data.frame(x = 3, y = 3.2)\np2 &lt;- ggplot(df, aes(x, y)) +\n  geom_point(color = \"black\", size = 3) +\n  geom_point(data = df_new_point, aes(x, y), color = \"green\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", formula = y ~ x) +\n  geom_line(data = spline_df, aes(x, y), color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Model Fit with New Point\", x = \"X\", y = \"Y\") +\n  theme_minimal()\n\n# Display side by side\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\nTwo side-by-side plots comparing model fits. The left plot shows four black data points with a blue linear regression line and a red dashed overfitted curve passing through all points. On the right plot, the original lines remain unchanged. There are the same 4 black data points plus a new green data point. The green point has the same x value as another point, but a different y value. The point lies far from the overfitted curve, but close to the linear regression line.\n\n\nAlthough the dotted line appeared to be an exceptional model for the data collected, it doesn’t adequately model the whole population. Some model diagnostics will not catch overfitting, and might even tell you that it is an amazing model. In the above example the \\(\\text{MSE} = 0\\). Which would be amazing. The problem is that this model isn’t flexible enough to accuratly predict future observations, nor to provide correct confidence intervals. It is overfitted to the data. We need a model that is more flexible to handle variance in the data.\n\nProblems\nLet’s discuss a couple potential problems leading to overfitting.\n\nToo many predictors\nMulticollinearity\n\n\nPredictors\nIn a world saturated with data, sometimes less is more. Especially when it comes to the number of predictors we use. As data scientists, we’re presensted with all the data collected. It is up to us to sift through the information available and to select the variables that are necessary to answer our questions. If we have to many predictor variables in our model we run the risk of finding significance that isn’t there. For example, if we are testing significance with \\(\\alpha \\le 0.05\\) and we test 20 insignificant variables. On average, one of those variables will appear to be statistically significant even though in reality it isn’t. Here’s a comic about that if you’re interested. When we use to many variables we can create a model where the coefficients tied to our variables match the current data set well, but will not match future data. We can sometimes even “take” some of the relationship from one variable and “give” it to another if the two variables are related. This is related to multicollinearity.\n\n\nMulticollinearity\nOne of my first realizations about multicollinearity is that I want to say this word as infrequently as possible. Luckily this is written and not a presentation. To understand multicollinearity, we must first understand that sometimes predictive variables can be related. For example, if I had a person intereted in contracting me to do data analytics for them and I wanted to determine how much I thought they’d be willing to pay me, I could consider both their income level and their house property value as predictors for the money they’d pay. The problem is that these two variables are often very related. Considering this mathematically, we see that the following equations are equivalent.\n$$ \\[\\begin{align}\n  \\begin{split}\n  \\text{Assume House} &\\text{Property Value} = 3*\\text{Income } \\\\\n  \\text{Money Paid}_1 &= (\\text{Property Value} - \\text{Income})/1000 \\\\\n  \\text{Money Paid}_2 &= (4 \\cdot \\text{Property Value} - 10 \\cdot \\text{Income})/1000 \\\\\n  \\text{Let Property Value} &= 300000 \\text{, Income} = 100000\\\\\n  \\text{Money Paid}_1 &= (300000 - 100000)/1000 = 200 \\\\\n  \\text{Money Paid}_2 &= (4 \\cdot 300000 - 10 \\cdot 100000)/1000 = 200 \\\\\n  \\end{split}\n\\end{align}\\]\n$$\nWe can see that, these 2 equations will produce the same results for a prediction of how much money a person will be willing to pay. This is due to a preexisting relationship between income level and house property value. So in this case, if your model were either of these is would produce the right value. The problem is that one of these equations will produce much different values if there is a slight change in Income or Property value. Since in the “wild” this relationship would often be a correlation and not an exact 3-1 relationship. This means that when making predictions for new data points, or in other words a new customer, your predictions could change rapidly despite small differences between them and an existing customer used to train the model. These 2 variables represent multicollinearity. In situations like these, it is generally better to only use one of your variables to include in the model.\n\n\n\nPotential Fixes\nIt is clear that these issues can be probematic. Today we will discuss two tools that can be used to help mitigate the chance of these problems. It is important to note that although these tools help, it is important to combine them with critical thinking and a personal understanding of your data set and modeling goals. These methods will help you to select potential models to use and to compare different potential models. I will give you enough information about these techniques to get started and to understand their potential. They are honestly interesting enough to have their own blog posts, so I will just focus on gerneal understanding here.\n\nRegularization Methods\nCross Validation\n\n\nRegularization Methods\nRegularization methods help us to shrink our coefficents to avoid inflated coefficients that increase the variablility induced by slight changes in the data. This can avoid the issue demonstrated in the multicollinearity example. These methods penalize large coefficients and help “push” them towards 0. Two examples of these techniques are Ridge and Lasso regression.\nRidge Regression - This is the method to choose when we don’t want to elimiate any variables, and just want to keep their coefficents low. For example, if we are trying to predict an animal’s lifespan based on water pollution. Two pollutants might be related to each other, but the environmental scientists might want to keep both pollutants in the model. This method helps to find that balance.\nLasso Regression - This is called a variable selection technique, since can actually cause a variable’s coefficient to be 0, thus eliminating the variable from the model. It, like Ridge regression, includes a penalty that “pushes” the coefficients toward 0. It can aid in both reducing inflation in variables and selecting variables to use. Both of these techniques can be used to help us as we combine the information they provide with our understanding of the problem. Many coding languages even include functions or packages that will help you use these tools!\n\n\nCross Validation\nCross validation is an essential aspect to model building. The purpose of cross validation is to check and see how well your model can predict future data. This is essential as a data scientist and a key to ensure that our model is not overfit to our data i.e. unflexible. For cross validation you will select the model you want to use, I would honestly recommend selecting multiple potential models here. You will then randomly split your data up into a training set and a test set. The model(s) will then be fit to the training set and then tested to see how well they model the test set. If it doesn’t do a good job predicting the test set, it is possible that the model is overfitting to your data. You should randomly split up the data multiple times and perform this process to then see how the model fits overall. I believe cross validation should be done everytime a model is being trained."
  },
  {
    "objectID": "projects/blog_post.html#conclusion",
    "href": "projects/blog_post.html#conclusion",
    "title": "How to fix (model) sensitivity problems",
    "section": "Conclusion",
    "text": "Conclusion\nThese two methods can even be combined and both used in the process of model selection and training. They will help us as data scientists to provide flexible yet accurate models to answer our questions and provide key insights to the problems presented to us. I implore you to check and ensure that your model is flexible and can handle the future data that will be given to it."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\n\nPast Projects\n\nMetal Fatigue Model (Bayesian Linear Regression, Prediction)\n\nPredicted metal breakage with 95% accuracy using a linear model\n\nPresented findings via presentation, executive summary, and report\n\nPerformed diagnostics and analysis to determine optimal model\n\nDesigned and implemented a 3-variable experiment to detect fatigue factors Click for repo\n\n\n\nStatistically Driven Best Spending Practices (Excel, ANOVA)\n\nReduced spending by 250% using multivariable statistical techniques\n\nIncreased study dependability by 200%+ through R-based procedures\n\nAuthored report and presented findings to peers to improve financial practices\n\n\n\n\nWork Experience\n\nTaxHawk Inc. — Product Development Specialist Intern\nProvo, UT\n- Reduced customer filing time by up to 8% by eliminating system redundancies\n- Completed pre-season product changes ahead of team; covered absent coworker’s tasks\n- Translated Rhode Island tax forms into pseudo-code for web developers\n\n\nBrigham Young University — Department of Statistics\nResearch Assistant\nProvo, UT\n- Coded algorithms for data analysis and visualizations of 3-level models\n- Halved code run time by optimizing languages and libraries\n- Consolidated codebase by 50% while maintaining scalability and flexibility\n\n\nBrigham Young University — Department of Mathematics\nResearch Assistant\nProvo, UT\n- Received top marks for work ethic and team contributions\n- Created weekly visualizations to enhance group understanding\n- Explored data and collaborated in small teams to meet weekly goals"
  },
  {
    "objectID": "about.html#who-am-i",
    "href": "about.html#who-am-i",
    "title": "About Me",
    "section": "Who am I?",
    "text": "Who am I?\nWhen I am not doing statistics or data science, I have a few hobbies that I enjoy. I play basketball regularly and play on an intramural team almost every semester. My wife and I love to dance together, so we will take dance classes at BYU, and will also go to social dance clubs in the area. When unwinding through a less physical activity, I enjoy watching sports and reading books."
  },
  {
    "objectID": "projects/data-acquisition.html#motivating-question",
    "href": "projects/data-acquisition.html#motivating-question",
    "title": "Data Acquisition",
    "section": "Motivating question",
    "text": "Motivating question\nWhat impact do race, sex, education, and iteractions between the factors have on a person’s chance of dying during the covid epidemic? Are any groups impacted by covid more than others in terms of a change in the proportion of deaths amoung younger people?",
    "crumbs": [
      "Data Acquisition"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#how-i-know-its-ethical",
    "href": "projects/data-acquisition.html#how-i-know-its-ethical",
    "title": "Data Acquisition",
    "section": "How I know it’s ethical",
    "text": "How I know it’s ethical\nI knew it was ethical through two processes. First, the data was taken from a government website that states that its purpose is to provide the public with data. The website also provided an API that would allow me to more easily scrape the data. Second, I checked the robots.txt and determined the rules and regulations that were placed on scraping and followed them.\n\nHow I gathered and prepared the data\nFirst, I found the webpage that contained the data I wanted and obtained an api that I used to scrape data onto my computer as a json file using the library “requests”. Then, I used the pandas library to convert the file into a dataframe. After that, I cleaned my data using regex to condense and clarify cell values. I dropped rows with unknown values, and I converted certain columns into numeric values so I could do math with them. After cleaning the data, I made a new dataset that I can use. This was done by condensing the dataset so that it contained the deaths for both 2019 and 2020 in the same row. I also determined the proportion of deaths (both total deaths and covid deaths) that occured for each age level while grouping by the demographic information. This was done by finding the total deaths per demographic subgroup and how many deaths occured for each age level. Finally, I also compared how the proportion of total deaths per age group in 2019 compared to the proportion of covid deaths per age group in 2020. This way, my 2019 proportional deaths data acts as a control when I analyze covid deaths.\n\n\nEDA highlights:\n\nSome basic summary statitics for some numeric values.\n\n\n\n\n\n\n\n\n\n\n\ncovid_19_deaths\nprop_dead_covid\nprop_dead_2019\nprop_change\n\n\n\n\ncount\n214\n214\n214\n214\n\n\nmean\n1743.07\n0.168224\n0.168224\n-1.94548e-19\n\n\nstd\n5754.05\n0.172858\n0.146603\n0.0504982\n\n\nmin\n0\n0\n0.00117369\n-0.178868\n\n\n25%\n33.5\n0.0224849\n0.0605526\n-0.0295447\n\n\n50%\n160\n0.123526\n0.137681\n-0.0104656\n\n\n75%\n907\n0.253918\n0.21127\n0.0295659\n\n\nmax\n59483\n0.791629\n0.72443\n0.190199\n\n\n\n\n\nA look at the average proportional change for each age level grouping on education level and race.\n\n\nRace\n\n\n\nrace_or_hispanic_origin\n0-24\n25-39\n40-54\n\n\n\n\nAm. Indian\n-0.0366441\n-0.037238\n-0.0118309\n\n\nAsian\n-0.0194403\n-0.0174561\n-0.0145093\n\n\nBlack\n-0.0236276\n-0.0333654\n-0.0187769\n\n\nHispanic\n-0.0316175\n-0.0369599\n0.00701345\n\n\nMultiracial\n-0.0683769\n-0.0645841\n-0.0378952\n\n\nWhite\n-0.00802079\n-0.0211336\n-0.0321271\n\n\n\n\n\nEducation\n\n\n\n\n\n\n\n\n\neducation_level\n0-24\n25-39\n40-54\n\n\n\n\nAssociate degree or some college\n-0.0202627\n-0.0447806\n-0.0151646\n\n\nBachelor’s degree or more\n-0.00436995\n-0.025146\n-0.019309\n\n\nHigh school graduate/GED or less\n-0.063852\n-0.0354419\n-0.0195895\n\n\n\n\n\nA deeper dive into the different proportional changes.\n\n\nEducation\n\n\n\nRace",
    "crumbs": [
      "Data Acquisition"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#what-are-the-most-interesting-findings-of-your-eda",
    "href": "projects/data-acquisition.html#what-are-the-most-interesting-findings-of-your-eda",
    "title": "Data Acquisition",
    "section": "What are the most interesting findings of your EDA?",
    "text": "What are the most interesting findings of your EDA?\nWhat’s most interesting to me is the lack of a clear difference in changes to proportion of deaths for different education levels. I thought that those with less education would be more susceptible to deaths from covid. Due to them having greater need to go work (less savings), and also jobs that are less likely to permit remote work. Both of these would have increased exposure to covid and in theory would have also increased deaths.",
    "crumbs": [
      "Data Acquisition"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#links-to-find-further-information",
    "href": "projects/data-acquisition.html#links-to-find-further-information",
    "title": "Data Acquisition",
    "section": "Links to find further information",
    "text": "Links to find further information\nAbout the data\nMy repo for the process",
    "crumbs": [
      "Data Acquisition"
    ]
  }
]